{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import gzip\n",
    "import cPickle as pickle\n",
    "import glob\n",
    "import csv\n",
    "\n",
    "import lightgbm as lgb\n",
    "import sklearn\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize, StandardScaler\n",
    "from scipy.sparse import coo_matrix, csr_matrix, load_npz\n",
    "\n",
    "import calendar\n",
    "import datetime, time\n",
    "import sys, os\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/torchsample-0.1.3-py2.7.egg/torchsample/modules/_utils.py:8: UserWarning: inspect.signature not available... you should upgrade to Python 3.x\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchsample\n",
    "from torchsample import TensorDataset\n",
    "from torchsample.modules import ModuleTrainer\n",
    "from torchsample.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from torchsample.regularizers import L1Regularizer, L2Regularizer\n",
    "from torchsample.constraints import UnitNorm\n",
    "from torchsample.initializers import XavierUniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "in_dir = \"/home/data/kaggle-zillow/processed/\"\n",
    "\n",
    "out_dir = \"/home/data/kaggle-zillow/submissions/\"\n",
    "\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load formatted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training data\n",
    "\n",
    "X = load_npz(in_dir + \"/X_train.npz\").tocsr()\n",
    "y = np.load(in_dir + \"/y_train.npz\")['arr_0']\n",
    "\n",
    "X_test = load_npz(in_dir + \"/X_test.npz\").tocsr()\n",
    "\n",
    "with np.load(in_dir + \"/features.npz\") as data:\n",
    "    features, features_dum = data['arr_0'][0], data['arr_0'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retaining 35 important features\n"
     ]
    }
   ],
   "source": [
    "# only retain important features as identified in prior analysis using gradient boosting\n",
    "# this is to reduce computational complexity, memory footprint, and variance\n",
    "\n",
    "RETAIN_TOP_FEATURES = 35\n",
    "\n",
    "if RETAIN_TOP_FEATURES:\n",
    "    features_imp = pd.read_csv(out_dir + \"/feature_importance.csv\")\n",
    "    features_imp.sort_values(\"importance\", inplace=True, ascending=False)\n",
    "    features = features_imp['feature'][:RETAIN_TOP_FEATURES]\n",
    "    idx_sel = [i for i,f in enumerate(features_dum)\\\n",
    "                   if len([x for x in features if x in f])>0]\n",
    "    features_dum = [features_dum[i] for i in idx_sel]\n",
    "    X, X_test = X[:,idx_sel], X_test[:,idx_sel]\n",
    "    \n",
    "    print \"Retaining %d important features\" % len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90275, 1443) (81247, 1443) (9028, 1443)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_eval, y_train, y_eval = train_test_split(X, y, \n",
    "                                        test_size=0.1, random_state=42)\n",
    "\n",
    "print X.shape, X_train.shape, X_eval.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ulimit = np.percentile(y_train, 99.5)\n",
    "llimit = np.percentile(y_train, 0.5)\n",
    "idx = (y_train > llimit) & (y_train < ulimit)\n",
    "X_train = X_train[idx,:]\n",
    "y_train = y_train[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((80430, 1443), (2985217, 1443))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize by dividing by standard deviation\n",
    "\n",
    "std_scaler = StandardScaler(with_mean=False)\n",
    "X_train = std_scaler.fit_transform(X_train)\n",
    "X_eval = std_scaler.transform(X_eval)\n",
    "\n",
    "# X_train = normalize(X_train, norm='l2', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a MLP in Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "\n",
    "X_train_tn = torch.from_numpy(X_train.todense()).double()\n",
    "y_train_tn = torch.from_numpy(y_train).double()\n",
    "X_eval_tn  = torch.from_numpy(X_eval.todense()).double()\n",
    "y_eval_tn  = torch.from_numpy(y_eval).double()\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tn, y_train_tn)\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=batch_size, \n",
    "                          shuffle=True)\n",
    "\n",
    "eval_dataset = TensorDataset(X_eval_tn, y_eval_tn)\n",
    "eval_loader = DataLoader(eval_dataset, \n",
    "                         batch_size=batch_size, \n",
    "                         shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define model and optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def define_mlp(input_size, hidden_sizes=[256,256]):\n",
    "    D = nn.Sequential(\n",
    "    nn.Linear(input_size, 128),\n",
    "    nn.LeakyReLU(0.2),\n",
    "#     nn.Linear(128, 256),\n",
    "#     nn.LeakyReLU(0.2),\n",
    "#     nn.Linear(256, 128),\n",
    "#     nn.LeakyReLU(0.2),\n",
    "#     nn.Linear(128, 128),\n",
    "#     nn.LeakyReLU(0.2),\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Linear(128, 1))\n",
    "    return D\n",
    "    \n",
    "    \n",
    "input_size = X_test.shape[1]\n",
    "learning_rate = 0.001\n",
    "    \n",
    "model = define_mlp(input_size)\n",
    "model = model.double()\n",
    "\n",
    "criterion = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferring to GPU\n"
     ]
    }
   ],
   "source": [
    "if torch.has_cudnn:\n",
    "    print(\"Transferring to GPU\")\n",
    "    X_train_tn = X_train_tn.cuda()\n",
    "    y_train_tn = y_train_tn.cuda()\n",
    "    X_eval_tn = X_eval_tn.cuda()\n",
    "    y_eval_tn = y_eval_tn.cuda()\n",
    "    model = model.cuda()\n",
    "    criterion = criterion.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> epoch: 0, batch 0, train error: 1.102054\n",
      ">>> epoch: 0, batch 100, train error: 0.111045\n",
      ">>> epoch: 0, batch 200, train error: 0.082850\n",
      ">>> epoch: 0, batch 300, train error: 0.070353\n",
      ">>> epoch: 0, test error: 0.086160\n",
      ">>> epoch: 1, batch 0, train error: 0.073622\n",
      ">>> epoch: 1, batch 100, train error: 0.062475\n",
      ">>> epoch: 1, batch 200, train error: 0.063458\n",
      ">>> epoch: 1, batch 300, train error: 0.075653\n",
      ">>> epoch: 1, test error: 0.073471\n",
      ">>> epoch: 2, batch 0, train error: 0.061452\n",
      ">>> epoch: 2, batch 100, train error: 0.054480\n",
      ">>> epoch: 2, batch 200, train error: 0.056791\n",
      ">>> epoch: 2, batch 300, train error: 0.055529\n",
      ">>> epoch: 2, test error: 0.071338\n",
      ">>> epoch: 3, batch 0, train error: 0.062262\n",
      ">>> epoch: 3, batch 100, train error: 0.063259\n",
      ">>> epoch: 3, batch 200, train error: 0.053788\n",
      ">>> epoch: 3, batch 300, train error: 0.053160\n",
      ">>> epoch: 3, test error: 0.070481\n",
      ">>> epoch: 4, batch 0, train error: 0.056448\n",
      ">>> epoch: 4, batch 100, train error: 0.055212\n",
      ">>> epoch: 4, batch 200, train error: 0.055680\n",
      ">>> epoch: 4, batch 300, train error: 0.053771\n",
      ">>> epoch: 4, test error: 0.069323\n",
      ">>> epoch: 5, batch 0, train error: 0.053650\n",
      ">>> epoch: 5, batch 100, train error: 0.049041\n",
      ">>> epoch: 5, batch 200, train error: 0.062788\n",
      ">>> epoch: 5, batch 300, train error: 0.063765\n",
      ">>> epoch: 5, test error: 0.069823\n",
      ">>> epoch: 6, batch 0, train error: 0.053413\n",
      ">>> epoch: 6, batch 100, train error: 0.059925\n",
      ">>> epoch: 6, batch 200, train error: 0.063990\n",
      ">>> epoch: 6, batch 300, train error: 0.055142\n",
      ">>> epoch: 6, test error: 0.069957\n",
      ">>> epoch: 7, batch 0, train error: 0.053814\n",
      ">>> epoch: 7, batch 100, train error: 0.053776\n",
      ">>> epoch: 7, batch 200, train error: 0.049911\n",
      ">>> epoch: 7, batch 300, train error: 0.055055\n",
      ">>> epoch: 7, test error: 0.069341\n",
      ">>> epoch: 8, batch 0, train error: 0.049481\n",
      ">>> epoch: 8, batch 100, train error: 0.064563\n",
      ">>> epoch: 8, batch 200, train error: 0.059714\n",
      ">>> epoch: 8, batch 300, train error: 0.066262\n",
      ">>> epoch: 8, test error: 0.069666\n",
      ">>> epoch: 9, batch 0, train error: 0.059678\n",
      ">>> epoch: 9, batch 100, train error: 0.057856\n",
      ">>> epoch: 9, batch 200, train error: 0.057517\n",
      ">>> epoch: 9, batch 300, train error: 0.059411\n",
      ">>> epoch: 9, test error: 0.071430\n",
      ">>> epoch: 10, batch 0, train error: 0.063142\n",
      ">>> epoch: 10, batch 100, train error: 0.058728\n",
      ">>> epoch: 10, batch 200, train error: 0.063064\n",
      ">>> epoch: 10, batch 300, train error: 0.052881\n",
      ">>> epoch: 10, test error: 0.071060\n",
      ">>> epoch: 11, batch 0, train error: 0.073856\n",
      ">>> epoch: 11, batch 100, train error: 0.064498\n",
      ">>> epoch: 11, batch 200, train error: 0.050575\n",
      ">>> epoch: 11, batch 300, train error: 0.054179\n",
      ">>> epoch: 11, test error: 0.069860\n",
      ">>> epoch: 12, batch 0, train error: 0.057313\n",
      ">>> epoch: 12, batch 100, train error: 0.058799\n",
      ">>> epoch: 12, batch 200, train error: 0.054706\n",
      ">>> epoch: 12, batch 300, train error: 0.051803\n",
      ">>> epoch: 12, test error: 0.070287\n",
      ">>> epoch: 13, batch 0, train error: 0.061807\n",
      ">>> epoch: 13, batch 100, train error: 0.065852\n",
      ">>> epoch: 13, batch 200, train error: 0.052482\n",
      ">>> epoch: 13, batch 300, train error: 0.054559\n",
      ">>> epoch: 13, test error: 0.069754\n",
      ">>> epoch: 14, batch 0, train error: 0.056060\n",
      ">>> epoch: 14, batch 100, train error: 0.063969\n",
      ">>> epoch: 14, batch 200, train error: 0.063306\n",
      ">>> epoch: 14, batch 300, train error: 0.059844\n",
      ">>> epoch: 14, test error: 0.069914\n",
      ">>> epoch: 15, batch 0, train error: 0.059250\n",
      ">>> epoch: 15, batch 100, train error: 0.055004\n",
      ">>> epoch: 15, batch 200, train error: 0.059280\n",
      ">>> epoch: 15, batch 300, train error: 0.050901\n",
      ">>> epoch: 15, test error: 0.069881\n",
      ">>> epoch: 16, batch 0, train error: 0.068462\n",
      ">>> epoch: 16, batch 100, train error: 0.058284\n",
      ">>> epoch: 16, batch 200, train error: 0.057530\n",
      ">>> epoch: 16, batch 300, train error: 0.052516\n",
      ">>> epoch: 16, test error: 0.070135\n",
      ">>> epoch: 17, batch 0, train error: 0.054539\n",
      ">>> epoch: 17, batch 100, train error: 0.054251\n",
      ">>> epoch: 17, batch 200, train error: 0.053758\n",
      ">>> epoch: 17, batch 300, train error: 0.052671\n",
      ">>> epoch: 17, test error: 0.069958\n",
      ">>> epoch: 18, batch 0, train error: 0.049362\n",
      ">>> epoch: 18, batch 100, train error: 0.060835\n",
      ">>> epoch: 18, batch 200, train error: 0.058362\n",
      ">>> epoch: 18, batch 300, train error: 0.061777\n",
      ">>> epoch: 18, test error: 0.069386\n",
      ">>> epoch: 19, batch 0, train error: 0.062893\n",
      ">>> epoch: 19, batch 100, train error: 0.052415\n",
      ">>> epoch: 19, batch 200, train error: 0.047874\n",
      ">>> epoch: 19, batch 300, train error: 0.056997\n",
      ">>> epoch: 19, test error: 0.072542\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in xrange(20):\n",
    "    # trainning\n",
    "    for batch_idx, (x, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        x, target = Variable(x.cuda()), Variable(target.cuda())\n",
    "        score = model(x)\n",
    "        loss = criterion(score, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            # clear_output(wait=True)\n",
    "            msg = '>>> epoch: {}, batch {}, train error: {:.6f}\\r'.format(epoch, batch_idx, loss.data[0])\n",
    "            print msg\n",
    "    # testing\n",
    "    ave_err = 0\n",
    "    for batch_idx, (x, target) in enumerate(eval_loader):\n",
    "        x, target = Variable(x.cuda(), volatile=True), Variable(target.cuda(), volatile=True)\n",
    "        score = model(x)\n",
    "        err = criterion(score, target)\n",
    "        ave_err += err.data[0]\n",
    "    ave_err /= len(eval_loader)\n",
    "    msg = '>>> epoch: {}, test error: {:.6f}\\r'.format(epoch, ave_err)\n",
    "    print msg\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions & format submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_nonz, j_nonz = X_test.nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_nonz = torch.DoubleTensor([X_test[i,j] for i,j in zip(i_nonz, j_nonz)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_sp = torch.sparse.DoubleTensor(X_test.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments using torchsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_dataset = TensorDataset(X_eval_tn, y_eval_tn)\n",
    "test_loader = DataLoader(eval_dataset, \n",
    "                         batch_size=batch_size, \n",
    "                         shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = ModuleTrainer(model)\n",
    "\n",
    "\n",
    "callbacks = [EarlyStopping(patience=10),\n",
    "             ReduceLROnPlateau(factor=0.5, patience=5)]\n",
    "# regularizers = [L1Regularizer(scale=1e-3, module_filter='*'),\n",
    "#                 L2Regularizer(scale=1e-5, module_filter='*')]\n",
    "constraints = [UnitNorm(frequency=3, unit='batch', module_filter='*')]\n",
    "initializers = [XavierUniform(bias=False, module_filter='*')]\n",
    "# metrics = [nn.L1Loss()]\n",
    "\n",
    "trainer.compile(loss=criterion,\n",
    "                optimizer='adam',\n",
    "                regularizers=regularizers,\n",
    "                # constraints=constraints,\n",
    "                # metrics=metrics,\n",
    "                initializers=initializers,\n",
    "                callbacks=callbacks)\n",
    "\n",
    "trainer.fit_loader(train_loader, eval_loader, num_epoch=20, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
